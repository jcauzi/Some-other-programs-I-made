{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment classifier with word-embeddings (exercise).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "A2BZykSRWlSN",
        "4YowcYrkUOwG",
        "qEOouXSvWiNA",
        "ikcb0jJiaotG",
        "RfdibF5dhMIh",
        "dIY-HdiPhgL4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcauzi/jcauzi/blob/main/Sentiment_classifier_with_word_embeddings_(exercise).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjtCaqGl1ofO"
      },
      "source": [
        "Exercice completed by Jules Cauzinille \n",
        "\n",
        "* I reached an accuracy of 0.87 on the dev set with the smallest embeddings (size 50)\n",
        "* I also added an early stopping criterion and plotted the accuracy evolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BZykSRWlSN"
      },
      "source": [
        "Goal\n",
        "==\n",
        "\n",
        "We are about to design and train a neural system to perform sentiment analysis on film reviews. More precisely, the network will have to output the probability that the input review expresses a positive opinion (overall).\n",
        "\n",
        "The system will be a bag-of-words model using GloVe embeddings. It will have to first average the embeddings of the words of the input review, and then send the result through a simple network that should output a probability.\n",
        "\n",
        "The first 5 parts were already implemented. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YowcYrkUOwG"
      },
      "source": [
        "Loading PyTorch\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dazbs4RArm"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOouXSvWiNA"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is the Large Movie Review Dataset (https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "Downloading the dataset and pre-processing it might take several minutes, so ask Colab to execute all cells while you are reading the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8AIWygRhI5"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "filename = tmp[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNSehWtRsXE"
      },
      "source": [
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097EOlPhS07G"
      },
      "source": [
        "# Extracts the dataset.\n",
        "import tarfile\n",
        "tar = tarfile.open(filename)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp3XCqJ5TS73"
      },
      "source": [
        "import os # Useful library to read files and inspect directories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOruARLhTU4e"
      },
      "source": [
        "# Shows which files and directories are present at the root of the file system.\n",
        "for filename in os.listdir(\".\"):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYr8dYnRtCx"
      },
      "source": [
        "dataset_root = \"aclImdb\"\n",
        "# Shows which files and directories are present at the root of the dataset directory.\n",
        "for filename in os.listdir(dataset_root):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM9B2NreR-MT"
      },
      "source": [
        "# Shows several reviews.\n",
        "dirname = os.path.join(dataset_root, \"train\", \"neg\") # \"aclImdb/{train|test}/{neg|pos}\"\n",
        "for idx, filename in enumerate(os.listdir(dirname)):\n",
        "  if(idx >= 5): break # Stops after the 5th file.\n",
        "  \n",
        "  print(filename)\n",
        "  with open(os.path.join(dirname, filename)) as f:\n",
        "    review = f.read()\n",
        "    print(review)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcb0jJiaotG"
      },
      "source": [
        "Preprocessing the dataset\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsLxzRGctTt"
      },
      "source": [
        "import nltk # Imports NLTK, an NLP library.\n",
        "nltk.download('punkt') # Loads a module required for tokenization.\n",
        "import collections # This library defines useful data structures. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWWSBBYUoto"
      },
      "source": [
        "newline = \"<br />\" # The reviews sometimes contain this HTLM tag to indicate a line break.\n",
        "def preprocess(text):\n",
        "  text = text.replace(newline, \" \") # Replaces the newline HTML tag with a space.\n",
        "  tokens = nltk.word_tokenize(text); # Converts the text to a list of tokens (strings).\n",
        "  tokens = [token.lower() for token in tokens] # Lowercases all tokens.\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "# Reads and pre-processes the reviews.\n",
        "dataset = {\"train\": [], \"test\": []}\n",
        "binary_classes = {\"neg\": 0, \"pos\": 1}\n",
        "for part_name, l in dataset.items():\n",
        "  for class_name, value in binary_classes.items():\n",
        "    path = os.path.join(dataset_root, part_name, class_name)\n",
        "    print(\"Processing %s...\" % path, end='');\n",
        "    for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename)) as f:\n",
        "          review_text = f.read()\n",
        "          review_tokens = preprocess(review_text)\n",
        "          \n",
        "          l.append((review_tokens, value))\n",
        "    print(\" done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDG9piYiVHL"
      },
      "source": [
        "# Splits the train set into a proper train set and a development/validation set.\n",
        "# 'dataset[\"train\"]' happens to be a list composed of a certain number of negative examples followed by the same number of positive examples.\n",
        "# We are going to use 3/4 of the original train set as our actual train set, and 1/4 as our development set.\n",
        "# We want to keep balanced train and development sets, i.e. for both, half of the reviews should be positive and half should be negative.\n",
        "if(\"dev\" in dataset): print(\"This should only be run once.\")\n",
        "else:\n",
        "  dev_set_half_size = int((len(dataset[\"train\"]) / 4) / 2) # Half of a quarter of the training set size.\n",
        "  dataset[\"dev\"] = dataset[\"train\"][:dev_set_half_size] + dataset[\"train\"][-dev_set_half_size:] # Takes some negative examples at the beginning and some positive ones at the end.\n",
        "  dataset[\"train\"] = dataset[\"train\"][dev_set_half_size:-dev_set_half_size] # Removes the examples used for the development set.\n",
        "\n",
        "  for (part, data) in dataset.items():\n",
        "    class_counts = collections.defaultdict(int)\n",
        "    for (_, p) in data: class_counts[p] += 1\n",
        "    print(f\"{part}: {class_counts}\")\n",
        "  print(\"Train set split into train/dev.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdibF5dhMIh"
      },
      "source": [
        "Loading the word embeddings\n",
        "==\n",
        "We are going to use GloVe embeddings.\n",
        "\n",
        "All word forms with a frequency below a given threshold are going to be considered unknown forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkKPGLYxQNR"
      },
      "source": [
        "# Computes the frequency of all word forms in the train set.\n",
        "word_counts = collections.defaultdict(int)\n",
        "for tokens, _ in dataset[\"train\"]:\n",
        "  for token in tokens: word_counts[token] += 1\n",
        "\n",
        "print(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgw19ofeZ23K"
      },
      "source": [
        "# Builds a vocabulary containing only those words present in the train set with a frequency above a given threshold.\n",
        "count_threshold = 4;\n",
        "vocabulary = set()\n",
        "for word, count in word_counts.items():\n",
        "    if(count > count_threshold): vocabulary.add(word)\n",
        "\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi6oZU1kph03"
      },
      "source": [
        "import zipfile\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnhHpcuxckbK"
      },
      "source": [
        "# Returns a dictionary {word[String]: id[Integer]} and a list of Numpy arrays.\n",
        "# `data_path` is the path of the directory containing the GloVe files (if None, 'glove.6B' is used)\n",
        "# `max_size` is the number of word embeddings read (starting from the most frequent; in the GloVe files, the words are sorted)\n",
        "# If `vocabulary` is specified, the output vocabulary contains the intersection \n",
        "#   of `vocabulary` and the words with a defined embedding. Otherwise, all words with a defined embedding are used.\n",
        "def get_glove(dim=50, vocabulary=None, max_size=-1, data_path=None):\n",
        "  dimensions = set([50, 100, 200, 300]) # Available dimensions for GloVe 6B\n",
        "  fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip' # (Remember that in GloVe 6B, words are lowercased.)\n",
        "\n",
        "  assert (dim in dimensions), (f'Unavailable GloVe 6B dimension: {dim}.')\n",
        "\n",
        "  if(data_path is None): data_path = 'glove.6B'\n",
        "\n",
        "  # Checks that the data is here, otherwise downloads it.\n",
        "  if(not os.path.isdir(data_path)):\n",
        "    #print('Directory \"%s\" does not exist. Creation.' % data_path)\n",
        "    os.makedirs(data_path)\n",
        "  \n",
        "  glove_weights_file_path = os.path.join(data_path, f'glove.6B.{dim}d.txt')\n",
        "  \n",
        "  if(not os.path.isfile(glove_weights_file_path)):\n",
        "    local_zip_file_path = os.path.join(data_path, os.path.basename(fallback_url))\n",
        "  \n",
        "    if(not os.path.isfile(local_zip_file_path)):\n",
        "      print(f'Retreiving GloVe embeddings from {fallback_url}.')\n",
        "      urllib.request.urlretrieve(fallback_url, local_zip_file_path)\n",
        "    \n",
        "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
        "      print(f'Extracting GloVe embeddings from {local_zip_file_path}.')\n",
        "      z.extractall(path=data_path)\n",
        "  \n",
        "  assert os.path.isfile(glove_weights_file_path), (f\"GloVe file {glove_weights_file_path} not found.\")\n",
        "\n",
        "  # Reads GloVe data.\n",
        "  print('Reading GloVe embeddings.')\n",
        "  new_vocabulary = {} # A dictionary {word[String]: id[Integer]}\n",
        "  embeddings = [] # The list of embeddings (Numpy arrays)\n",
        "  with open(glove_weights_file_path, 'r') as f:\n",
        "    for line in f: # Each line consist of the word followed by a space and all of the coefficients of the vector separated by a space.\n",
        "      values = line.split()\n",
        "\n",
        "      # Here, I'm trying to detect where on the line the word ends and where the vector begins.\n",
        "      #   As in some version(s) of GloVe words can contain spaces, this is not entirely trivial.\n",
        "      vector_part = ' '.join(values[-dim:])\n",
        "      x = line.find(vector_part)\n",
        "      word = line[:(x - 1)]\n",
        "\n",
        "      if((vocabulary is not None) and (not word in vocabulary)): # If a vocabulary was specified and if the word is not it…\n",
        "        continue # …this word is skipped.\n",
        "\n",
        "      new_vocabulary[word] = len(new_vocabulary)\n",
        "      embedding = np.asarray(values[-dim:], dtype=np.float32)\n",
        "      embeddings.append(embedding)\n",
        "\n",
        "      if(len(new_vocabulary) == max_size): break\n",
        "  print('(GloVe embeddings loaded.)')\n",
        "  print()\n",
        "\n",
        "  return (new_vocabulary, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9nJNVlpZiL"
      },
      "source": [
        "(new_vocabulary, embeddings) = get_glove(dim=50, vocabulary=vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaov0tWpeFZ"
      },
      "source": [
        "print(len(new_vocabulary)) # Shows the size of the vocabulary.\n",
        "print(new_vocabulary) # Shows each word and its id.\n",
        "#print l'embedding du mot \"and\" :\n",
        "# print(type(embeddings))\n",
        "# print(new_vocabulary[\"and\"])\n",
        "# print(embeddings[new_vocabulary[\"and\"]])\n",
        "\n",
        "# unk = np.average(embeddings, axis=0)\n",
        "# print(unk)\n",
        "# pad = np.zeros_like(unk)\n",
        "# print(pad)\n",
        "\n",
        "embeddings_tensor = torch.FloatTensor(embeddings)\n",
        "print(embeddings_tensor.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIY-HdiPhgL4"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07xfo-Gux_X"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "class BatchGenerator:\n",
        "  def __init__(self, dataset, vocabulary):\n",
        "    self.dataset = dataset\n",
        "    for part in self.dataset.values(): # Shuffles the dataset so that positive and negative examples are mixed.\n",
        "      np.random.shuffle(part)\n",
        "\n",
        "    self.vocabulary = vocabulary # Dictonary {word[String]: id[Integer]}\n",
        "    self.unknown_word_id = len(vocabulary) # Id for unknown forms\n",
        "    self.padding_idx = len(vocabulary) + 1 # Not all reviews of a given batch will have the same length.\n",
        "    #   We will \"pad\" shorter reviews with a special token id so that the batch can be represented by a matrix.\n",
        "  \n",
        "  def length(self, data_type='train'):\n",
        "    return len(self.dataset[data_type])\n",
        "\n",
        "  # Returns a random batch.\n",
        "  # Batches are output as a triples (word_ids, polarity, texts). \n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def get_batch(self, batch_size, data_type, subset=None):\n",
        "    data = self.dataset[data_type] # selects the relevant portion of the dataset.\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "    instance_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some instance ids.\n",
        "\n",
        "    return self._ids_to_batch(data, instance_ids)\n",
        "\n",
        "  def _ids_to_batch(self, data, instance_ids):\n",
        "    word_ids = [] # Will be a list of lists of word ids (Integer)\n",
        "    polarity = [] # Will be a list of review polarities (Boolean)\n",
        "    texts = [] # Will be a list of lists of words (String)\n",
        "    for instance_id in instance_ids:\n",
        "      text, p = data[instance_id]\n",
        "\n",
        "      word_ids.append([self.vocabulary.get(w, self.unknown_word_id) for w in text])\n",
        "      polarity.append(p)\n",
        "      texts.append(text)\n",
        "    \n",
        "    # Padding\n",
        "    self.pad(word_ids)\n",
        "\n",
        "    word_ids = torch.tensor(word_ids, dtype=torch.long) # Conversion to a tensor\n",
        "    polarity = torch.tensor(polarity, dtype=torch.bool) # Conversion to a tensor\n",
        "\n",
        "    return (word_ids, polarity, texts) # We don't really need `texts` but it might be useful to debug the system.\n",
        "  \n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, word_ids):\n",
        "    max_length = max([len(s) for s in word_ids])\n",
        "    for s in word_ids: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "  \n",
        "  # Returns a generator of batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, data_type=\"train\", subset=None):\n",
        "    data = self.dataset[data_type]\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size')\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "      i += batch_size\n",
        "    \n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "  \n",
        "  # Turns a list of arbitrary pre-processed texts into a batch.\n",
        "  # This function will be used to infer the polarity of a unannotated review.\n",
        "  def turn_into_batch(self, texts):\n",
        "    word_ids = [[self.vocabulary.get(w, self.unknown_word_id) for w in text] for text in texts]\n",
        "    self.pad(word_ids)\n",
        "    return torch.tensor(word_ids, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(dataset=dataset, vocabulary=new_vocabulary)\n",
        "print(batch_generator.length('train')) # Prints the number of instance in the train set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35rEb8l0_Kd"
      },
      "source": [
        "tmp = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(tmp[0]) # Prints the matrix of token ids.\n",
        "print(tmp[1]) # Prints the vector of polarities.\n",
        "print(tmp[2]) # Prints the list of reviews.\n",
        "\n",
        "print(type(batch_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp2VOTBzFLb9"
      },
      "source": [
        "len(list(batch_generator.all_batches(batch_size=3, data_type=\"train\"))) # Number of batches in the training set for batches of size 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTuIZoIhkTW"
      },
      "source": [
        "The model\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6mbJ9Q2nUy"
      },
      "source": [
        "class SentimentClassifier(torch.nn.Module):\n",
        "  # embeddings: list of Numpy arrays\n",
        "  # hidden_sizes: list of the size (Integer) of each hidden layer; there may be 0 or more hidden layers\n",
        "  def __init__(self, embeddings, hidden_sizes, freeze_embeddings=True, device='cpu'):\n",
        "    embeddings = list(embeddings) # Creates a copy of the list of embeddings, so we can add or remove entries without affecting the original list.\n",
        "    super().__init__() # Calls the constructor of the parent class. Usually, this is necessary when creating a custom module.\n",
        "\n",
        "    self.padding_idx = len(embeddings) + 1 # len(embeddings) will be the id of the embedding of the unknown word\n",
        "\n",
        "    # Here you have to (i) define a vector for unknown forms (the average of actual word embeddings)\n",
        "    # and a vector for the padding token (full of 0·s) and (ii) define an embedding layer 'self.embeddings' using torch.nn.Embedding.from_pretrained\n",
        "    # and without forgeting to use the 'freeze' and 'padding_idx' arguments.\n",
        "\n",
        "    #i\n",
        "\n",
        "    self.unk = np.average(embeddings, axis=0)#vecteur comme moyenne de tous les embeddings.\n",
        "    self.padding = np.zeros_like(self.unk)#vecteur de 0 de la même taille que les embeddings\n",
        " \n",
        "    #ii\n",
        "    embeddings.append(self.unk)\n",
        "    embeddings.append(self.padding)\n",
        "    embeddings_tensor = torch.FloatTensor(embeddings)\n",
        "    self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, freeze=freeze_embeddings, padding_idx=self.padding_idx)\n",
        "\n",
        "    self.embeddings = self.embeddings.to(device) # Sends the word embeddings to 'device', which is potentially a GPU.\n",
        "\n",
        "    # Here you have to define self.main_part, the network that computes a probability for any review given as input\n",
        "    # (represented as the average of the embeddings of the tokens).\n",
        "    # The number of hidden layers is determined by 'hidden_sizes', which is a list of integers describing the (output) size of each of them.\n",
        "    # Use torch.nn.Linear to build linear layers.\n",
        "    # torch.nn.Sequential takes one argument per module and not a list of modules as argument, but if 'modules' is a list of modules,\n",
        "    # 'torch.nn.Sequential(*modules)' (with the star notation) works.\n",
        "    \n",
        "    modules = []\n",
        "\n",
        "    #création / ajout de la première couche cachée\n",
        "    modules.append(torch.nn.Linear(len(embeddings[0]), hidden_sizes[0]))\n",
        "\n",
        "    #création / ajout des fonctions d'activation + autres couches potentielles\n",
        "    for i in range(1, len(hidden_sizes)) :\n",
        "        activation = torch.nn.Tanh()\n",
        "        modules.append(activation)\n",
        "        layer = torch.nn.Linear(hidden_sizes[i-1], hidden_sizes[i])\n",
        "        modules.append(layer)\n",
        "\n",
        "    #création / ajout de la couche + fonction de sortie\n",
        "    modules.append(torch.nn.Linear(hidden_sizes[-1], 1))\n",
        "    modules.append(torch.nn.Sigmoid())\n",
        "\n",
        "    #print(modules)\n",
        "\n",
        "    #construction du réseau à partir de la liste de couches cachées \"modules\"\n",
        "    self.main_part = torch.nn.Sequential(*modules)\n",
        "    \n",
        "    self.main_part = self.main_part.to(device) # Sends the network to 'device', which is potentially a GPU.\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  # 'batch' is a matrix of word ids (Integer).\n",
        "\n",
        "  def forward(self, batch):\n",
        "    # Here you have to (i) turn 'batch' into a matrix of embeddings (i.e. a tensor of rank 3), (ii) average all embeddings for a given review\n",
        "    # while being careful not to take into account padding vectors, (iii) send these bag-of-words representations to the network.\n",
        "    # Return a tensor of shape (batch size) instead of (batch size, 1).\n",
        "    # Once you think the function works, check that the presence of padding ids do NOT impact the result in any way\n",
        "    # (i.e. the same probability should be computed for a given review independently of the number of padding ids).\n",
        "    #################\n",
        "\n",
        "    #i turn 'batch' into a matrix of embeddings (tensor of rank 3)\n",
        "\n",
        "    embedded_batch = self.embeddings(batch)\n",
        "    \n",
        "    #ii average embeddings without padding vectors\n",
        "\n",
        "    lenghts = batch \n",
        "    #lenghts est une matrice qui, pour chaque review, contient sa longueur en mots\n",
        "    lenghts = (lenghts != self.padding_idx).sum(dim=1)\n",
        "\n",
        "    #mean est la matrice contenant les embeddings \"moyennés\" de tous les mots d'une review (sans les pads)\n",
        "    #Sachant que les embeddings de pad sont vides, la somme des embeddings d'un exemple ne les prendra pas en compte \n",
        "    #En divisant cette somme par le nb de mots n'étant pas des pads, on obtient la moyenne sans padding.\n",
        "    mean = torch.sum(embedded_batch,1) / lenghts.unsqueeze(1)\n",
        "\n",
        "    # #iii send the bow to the network\n",
        "    \n",
        "    return self.main_part(mean).squeeze(1)\n",
        "\n",
        "    #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpqcxScW4Afb"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[100], freeze_embeddings=True)\n",
        "batch = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(\"input :\", model(batch[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmL7aPacmNVh"
      },
      "source": [
        "# Function that computes the accuracy of the model on a given part of the dataset.\n",
        "evaluation_batch_size = 256\n",
        "def evaluation(data_type, subset=None):\n",
        "  nb_correct = 0\n",
        "  total = 0\n",
        "  for batch in batch_generator.all_batches(evaluation_batch_size, data_type=data_type, subset=subset):\n",
        "    prob = model(batch[0].to(model.device)) # Forward pass\n",
        "    answer = (prob > 0.5) # Shape: (batch_size, 1)\n",
        "    nb_correct += (answer == batch[1].to(model.device)).sum().item()\n",
        "    total += batch[0].shape[0]\n",
        "      \n",
        "  accuracy = (nb_correct / total)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh9Z_UDwpdVN"
      },
      "source": [
        "Accuracy evolution plot\n",
        "==\n",
        "Function to plot the evolution of train and dev accuracy after training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrYqDMlNpc8p"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(train_accuracies, dev_accuracies) :\n",
        "  plt.plot(train_accuracies, label='Train Accuracy')\n",
        "  plt.plot(dev_accuracies, label='Dev Accuracy')\n",
        "  plt.title('Training and Validation accuracy evolution')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_gHaFzrOaj"
      },
      "source": [
        "Training\n",
        "==\n",
        "Once everything works, try to find better hyperparameters.\n",
        "The goal is to maximise the accuracy on the development set.\n",
        "Feel also free to improve the model or the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNJXvCrnpNH5"
      },
      "source": [
        "#parts surrounded by ########## correpond to the early stopping \n",
        "\n",
        "model = SentimentClassifier(embeddings, hidden_sizes=[100, 200, 20], freeze_embeddings=False, device='cuda')\n",
        "\n",
        "# Tests the model on a couple of instance before training.\n",
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "#print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.004\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg) # Once the backward propagation has been done,\n",
        "# just call the 'step' method (with no argument) of this object to update the parameters.\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 40\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "\n",
        "##########\n",
        "earlyStopping = True #Condition to early stop the training according to the following parameter :\n",
        "patience = 2\n",
        "#variables used for early stopping :\n",
        "last_accuracy = 0\n",
        "trigger_times = 0\n",
        "##########\n",
        "\n",
        "train_accuracies = []\n",
        "dev_accuracies = []\n",
        "\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells PyTorch that we are in training mode (can be useful if dropout is used, for instance).\n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset)\n",
        "\n",
        "  # (i) compute the prediction of the model (you might want to use \".to(model.device)\" on the input of the model),\n",
        "  outputs = model(batch[0].to(model.device))\n",
        "\n",
        "  # (ii) compute the loss (use an average over the batch),\n",
        "  loss_function = torch.nn.BCELoss(reduction=\"mean\")\n",
        "  target = batch[1].float()\n",
        "  loss = loss_function(outputs, target.to(model.device))\n",
        "\n",
        "  # (iii) call \"backward\" on the loss and\n",
        "  loss.backward()\n",
        "\n",
        "  # (iv) store the loss in \"epoch_loss\".\n",
        "  epoch_loss.append(loss)\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size):\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    if (len(epoch_loss) != 0) :\n",
        "      print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      accuracy = evaluation(\"train\")\n",
        "      print(f\"Accuracy on the train set: {accuracy}.\")\n",
        "      train_accuracies.append(accuracy)\n",
        "\n",
        "      accuracy = evaluation(\"dev\")\n",
        "      print(f\"Accuracy on the dev set: {accuracy}.\")\n",
        "      dev_accuracies.append(accuracy)\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []\n",
        "\n",
        "    ##########\n",
        "    #early stopping on the validation accuracy\n",
        "\n",
        "    if earlyStopping and epoch_id>10 :  #didn't find a better way to avoid an excessively early stopping (especially for low patience)\n",
        "                                        #(usually 10 epochs is the bare minimum to train the model correctly)\n",
        "      if accuracy < last_accuracy : \n",
        "        #print(accuracy, last_accuracy, \"Trigger !\")\n",
        "        trigger_times += 1\n",
        "      else :\n",
        "        #print(\"untrigger\")\n",
        "        trigger_times = 0\n",
        "        #saving optimal parameters so far\n",
        "        if accuracy > max(dev_accuracies) :\n",
        "          best_param = model.state_dict()\n",
        "\n",
        "      if trigger_times >= patience :\n",
        "        print(f\"accuracy went down {trigger_times} times ! \\nTraining early stopped.\")\n",
        "        break\n",
        "    \n",
        "      last_accuracy = accuracy\n",
        "      ##########\n",
        "\n",
        "#resetting the model on the best parameters :\n",
        "model.load_state_dict(best_param)\n",
        "print(f\"Best epoch is #{dev_accuracies.index(max(dev_accuracies))+1} for a dev accuracy of {max(dev_accuracies)}\")\n",
        "#(using this weird index method because saving the best epoch_id doesn't seem to be working...)\n",
        "plot(train_accuracies, dev_accuracies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4na1hivgXP"
      },
      "source": [
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "test_reviews = [\"This movie was terrible!!\", \"Pure gold!\", \"Bad.\", \"Not bad!\", \"I loved it\", \"it's alright\", \"Useless movie, I wish I could unsee it\"]\n",
        "test = model(batch_generator.turn_into_batch([preprocess(text) for text in test_reviews]).to(model.device))\n",
        "bool_test = test>0.5\n",
        "\n",
        "#pretty print of inputs / class\n",
        "for i in range(len(test_reviews)) :\n",
        "  out = \"+\" if bool_test[i].item() else \"-\"\n",
        "  print(test_reviews[i], \":\", out)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}