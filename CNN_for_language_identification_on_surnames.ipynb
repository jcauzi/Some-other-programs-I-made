{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN for language identification on surnames",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcauzi/jcauzi/blob/main/CNN_for_language_identification_on_surnames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZNxC0vJwrrQ"
      },
      "source": [
        "M2 project : Convolutions and character embeddings\n",
        "======================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKsopio6xxUt"
      },
      "source": [
        "The project aims to predict the language from which a character sequence comes from. This is done with surnames and it involves a dozen of languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ4ilt0wwqV8"
      },
      "source": [
        "Data download & description\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uBeXkKQxDrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7edb04-73cb-42a6-e4f4-4f28b06bfa0b"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('http://www.linguist.univ-paris-diderot.fr/~bcrabbe/datasets/name2lang.train','name2lang.train')\n",
        "urlretrieve('http://www.linguist.univ-paris-diderot.fr/~bcrabbe/datasets/name2lang.valid','name2lang.valid')\n",
        "\n",
        "#Prints the beginning of the valid set\n",
        "istream = open('name2lang.valid')\n",
        "for idx, line in enumerate(istream):\n",
        "  print(line.strip())\n",
        "  if idx >=20:\n",
        "    break\n",
        "istream.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barros, Portuguese\n",
            "Campos, Portuguese\n",
            "D'cruz, Portuguese\n",
            "Henriques, Portuguese\n",
            "Machado, Portuguese\n",
            "Silva, Portuguese\n",
            "Torres, Portuguese\n",
            "Ahearn, Irish\n",
            "Aonghus, Irish\n",
            "Brady, Irish\n",
            "Cearbhall, Irish\n",
            "Flann, Irish\n",
            "Kavanagh, Irish\n",
            "Maguire, Irish\n",
            "Mcmahon, Irish\n",
            "Mcneil, Irish\n",
            "Monahan, Irish\n",
            "Muirchertach, Irish\n",
            "Mullen, Irish\n",
            "O'Connell, Irish\n",
            "O'Grady, Irish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3W1SX1KCLwI"
      },
      "source": [
        "First exercise : data preprocessing (3pts)\n",
        "---\n",
        "The first exercise amounts to create encodings from integers to strings and from strings to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsrR4i9NCKni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda09c15-47ca-455e-9d12-31647ec51468"
      },
      "source": [
        "def vocabulary(filename,char_vocab,pad_token='<pad>'):\n",
        "    #char_vocab is a boolean flag that tells if we extract char symbols or language codes\n",
        "    #TODO : return the two encoding maps idx2sym and sym2idx as a couple\n",
        "    istream = open(filename)\n",
        "\n",
        "    #For characters (if boolean flag)\n",
        "    if char_vocab :\n",
        "      idx2sym = [pad_token]\n",
        "      for line in istream:\n",
        "        for char_symbol in line.split(\",\")[0] :\n",
        "          if char_symbol not in idx2sym :\n",
        "            idx2sym.append(char_symbol)\n",
        "\n",
        "    #For languages (if not boolean flag)\n",
        "    else : \n",
        "      idx2sym = []\n",
        "      for line in istream:\n",
        "        language = line.split(\",\")[1].strip()\n",
        "        if language not in idx2sym :\n",
        "          idx2sym.append(language)\n",
        "    \n",
        "    sym2idx = {char: i for i, char in enumerate(idx2sym)}\n",
        "\n",
        "    return idx2sym, sym2idx\n",
        "\n",
        "print(vocabulary('name2lang.valid', True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['<pad>', 'B', 'a', 'r', 'o', 's', 'C', 'm', 'p', 'D', \"'\", 'c', 'u', 'z', 'H', 'e', 'n', 'i', 'q', 'M', 'h', 'd', 'S', 'l', 'v', 'T', 'A', 'g', 'y', 'b', 'F', 'K', 't', 'O', 'G', 'R', 'f', ' ', 'E', 'x', 'L', 'N', 'P', 'U', 'V', 'Z', 'w', 'Q', 'J', 'k', 'j', 'W', 'I', 'Y'], {'<pad>': 0, 'B': 1, 'a': 2, 'r': 3, 'o': 4, 's': 5, 'C': 6, 'm': 7, 'p': 8, 'D': 9, \"'\": 10, 'c': 11, 'u': 12, 'z': 13, 'H': 14, 'e': 15, 'n': 16, 'i': 17, 'q': 18, 'M': 19, 'h': 20, 'd': 21, 'S': 22, 'l': 23, 'v': 24, 'T': 25, 'A': 26, 'g': 27, 'y': 28, 'b': 29, 'F': 30, 'K': 31, 't': 32, 'O': 33, 'G': 34, 'R': 35, 'f': 36, ' ': 37, 'E': 38, 'x': 39, 'L': 40, 'N': 41, 'P': 42, 'U': 43, 'V': 44, 'Z': 45, 'w': 46, 'Q': 47, 'J': 48, 'k': 49, 'j': 50, 'W': 51, 'I': 52, 'Y': 53})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl69X5Pmh_0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2b7a8c-2e79-4b6c-8004-78512605ff1a"
      },
      "source": [
        "def pad_sequence(sequence,pad_size,pad_token):\n",
        "    #returns a list with additional pad tokens to match pad_size if needed\n",
        "    pads_needed = pad_size - len(sequence)\n",
        "    return sequence + [pad_token] * pads_needed\n",
        "\n",
        "##TEST\n",
        "#print(pad_sequence([\"y\", \"n\"], 5, \"pad\"))\n",
        "\n",
        "def code_sequence(charseq,encodingmap):\n",
        "  #we ignore chars not seen in train set\n",
        "  #charseq is a sequence of chars\n",
        "  return [encodingmap[c] for c in charseq if c in encodingmap]\n",
        "\n",
        "##TEST\n",
        "coded = code_sequence(['a', 'b', 'c'], vocabulary('name2lang.valid', True)[1])\n",
        "print(coded)\n",
        "\n",
        "def decode_sequence(idxseq,decodingmap):\n",
        "  #idxseq is a list of integers\n",
        "  return [decodingmap[idx] for idx in idxseq]\n",
        "\n",
        "##TEST\n",
        "print(decode_sequence(coded, vocabulary('name2lang.valid', True)[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 29, 11]\n",
            "['a', 'b', 'c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShCgpvAoaK4S"
      },
      "source": [
        "Second exercise : data generator (2pt)\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIjvldl_zMJl"
      },
      "source": [
        "The data generator aims to deliver efficiently well formed batches of data to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OUau0XlbKmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b58829-31eb-4659-ce62-67b06107493a"
      },
      "source": [
        "def read_dataset(filename,input_symbols):\n",
        "    #reads from a raw datafile, either the surnmaes if input_symbols is True otherwise it reads the language \n",
        "    symbols = []\n",
        "    istream = open(filename)\n",
        "    for line in istream:\n",
        "      if line and not line.isspace():\n",
        "        word,lang = line.split(',')\n",
        "        symbol = list(word.strip()) if input_symbols else lang.strip()\n",
        "        symbols.append(symbol)\n",
        "    istream.close()\n",
        "    return symbols\n",
        "\n",
        "print(len(read_dataset('name2lang.valid', True)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ErFzoBuaYQz"
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "class DataGenerator:\n",
        "\n",
        "      def __init__(self,filename, parentgenerator = None,pad_token='<pad>'):\n",
        "\n",
        "           self.pad_token = pad_token\n",
        "           if parentgenerator is not None: #Reuse the encodings of the parent if specified\n",
        "             self.input_idx2sym,self.input_sym2idx     = parentgenerator.input_idx2sym,parentgenerator.input_sym2idx\n",
        "             self.output_idx2sym,self.output_sym2idx   = parentgenerator.output_idx2sym,parentgenerator.output_sym2idx\n",
        "           else:\n",
        "             #######################################\n",
        "             self.input_idx2sym,self.input_sym2idx     = vocabulary(filename, True, pad_token)\n",
        "             self.output_idx2sym,self.output_sym2idx   = vocabulary(filename, False, pad_token)\n",
        "             ####################################\n",
        "           self.X = read_dataset(filename,True)\n",
        "           self.Y = read_dataset(filename,False)\n",
        "\n",
        "      def generate_batches(self,batch_size):\n",
        "         \n",
        "              assert(len(self.X) == len(self.Y))\n",
        "              \n",
        "              N     = len(self.X)\n",
        "              idxes = list(range(N))\n",
        "\n",
        "              #Data ordering (try to explain why these 2 lines make sense...)\n",
        "              shuffle(idxes)\n",
        "              idxes.sort(key=lambda idx: len(self.X[idx]))\n",
        "              #generates a different distribution of examples each time :\n",
        "              #without shuffle(), examples would always be sorted in the same ordrer \n",
        "\n",
        "              #batch generation\n",
        "              bstart = 0\n",
        "              while bstart < N:\n",
        "                 bend        = min(bstart+batch_size,N)\n",
        "                 batch_idxes = idxes[bstart:bend] \n",
        "                 batch_len   = max(len(self.X[idx]) for idx in batch_idxes)   \n",
        "                 Xpad        = [pad_sequence(self.X[idx],batch_len,self.pad_token)  for idx in batch_idxes]               \n",
        "                 seqX        = [code_sequence(x,self.input_sym2idx) for x in Xpad]\n",
        "                 seqY        = [self.output_sym2idx[self.Y[idx]] for idx in batch_idxes]\n",
        "\n",
        "                 assert(len(seqX) == len(seqY))\n",
        "                 yield (seqX,seqY)\n",
        "                 bstart += batch_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuydskNI5LVG"
      },
      "source": [
        "Third exercise : Implement the word embedding submodule (5pts)\n",
        "-----\n",
        "This exercise amounts to implement a pytorch submodule that takes as input a sequence of char indexes and outputs the word embedding corresponding for the sequence.\n",
        "\n",
        "The module contains no training method and is meant to be used in a larger network. Its use is quite similar to `nn.Embedding`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it7vLjW76HfS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CharConvolution(nn.Module):\n",
        "\n",
        "      def __init__(self,windowK,chars_vocab_size,input_embedding_size,output_embedding_size,padding_idx = None):\n",
        "\n",
        "          super(CharConvolution, self).__init__()\n",
        "          #####################################\n",
        "          #Initialize and allocate an embeddings class and the Conv1d class\n",
        "          self.embeddings = nn.Embedding(chars_vocab_size, input_embedding_size, padding_idx=padding_idx)\n",
        "          self.convlayer = nn.Conv1d(input_embedding_size, output_embedding_size, (2*windowK)+1, padding=windowK)\n",
        "          #####################################\n",
        "\n",
        "      def forward(self,xinput):\n",
        "          #####################################\n",
        "          #Implement the forward method, taking an input of the form [batch,seq]\n",
        "          #and return the max pooled result\n",
        "          embedded_xinput = self.embeddings(xinput)\n",
        "          embedded_xinput = torch.transpose(embedded_xinput, 1, 2)\n",
        "          batch_size,embedding_size,nsymbols = embedded_xinput.shape\n",
        "          pool = nn.MaxPool1d(nsymbols)\n",
        "          return pool(self.convlayer(embedded_xinput)).squeeze()\n",
        "          ######################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSzp5yYuaNHk"
      },
      "source": [
        "Fourth Exercise : predict the target language (10pts)\n",
        "-------\n",
        "In this exercise, we aim to predict the target language from a word char embedding. You will implement for the `LanguageIdentifier` class:\n",
        "* A forward function: the function takes as input a char index tensor and returns a vector of prediction for each word\n",
        "* A train function: the function trains the model on the full dataset (with early stopping)\n",
        "* A predict function: the function takes a test corpus (a list of words)\n",
        "and predicts the language. The function outputs its results in textual form. Each word is printed on the same line as its predicted class.\n",
        "\n",
        "Once implemented you are expected to search for hyperparameters in the main program.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf3OJm0_bB3O"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class LanguageIdentifier(nn.Module):\n",
        "\n",
        "    def __init__(self,datagenerator,window_size,char_embedding_size,word_embedding_size):\n",
        "      super(LanguageIdentifier, self).__init__()       \n",
        "      invocab_size   = len(datagenerator.input_idx2sym)\n",
        "      outvocab_size  = len(datagenerator.output_idx2sym)\n",
        "      pad_idx        = datagenerator.input_sym2idx[datagenerator.pad_token]\n",
        "      self.charE     = CharConvolution(window_size,invocab_size,char_embedding_size,word_embedding_size,padding_idx = pad_idx)\n",
        "      self.output    = nn.Linear(word_embedding_size,outvocab_size)\n",
        "      # self.class_vocab = vocabulary('name2lang.valid', False)[0]\n",
        "      # self.letter_vocab = vocabulary('name2lang.valid', True)[0]\n",
        "\n",
        "    def load(self,filename):\n",
        "        self.load_state_dict(torch.load(filename))\n",
        "\n",
        "\n",
        "    def forward(self,xinput):\n",
        "      #########################\n",
        "      #takes as input a tensor of the form [batch,seq] \n",
        "      #and returns a vector of predictions for the language \n",
        "      #print(\"input shape :\", xinput.shape )\n",
        "      conv = self.charE(xinput)\n",
        "      #print(\"conv shape : \", conv.shape)\n",
        "      return self.output(conv)\n",
        "      #########################\n",
        "\n",
        "    def train(self,traingenerator,validgenerator,epochs,batch_size,device='cpu',learning_rate=0.001):\n",
        "\n",
        "      self.minloss = 10000000 #the minimal validation loss found so far for an epoch     \n",
        "      ###########################\n",
        "      #TODO Implement the training function, save the model with minimum loss\n",
        "\n",
        "      optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "      ##Earlystopping :\n",
        "      trigger_times = 0\n",
        "      patience = 3\n",
        "      previous_loss = self.minloss\n",
        "      all_losses = []\n",
        "      ##\n",
        "\n",
        "      device = torch.device(device)\n",
        "      loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "      for epoch in range(epochs) :\n",
        "          self.zero_grad()\n",
        "          batch_accurracies = []\n",
        "          batch_losses      = []\n",
        "          batch_sizes       = []\n",
        "          print(epoch)\n",
        "\n",
        "          for (trainX, trainY) in traingenerator.generate_batches(batch_size):\n",
        "            X    = torch.LongTensor(trainX).to(device)\n",
        "            Y    = torch.LongTensor(trainY).to(device)\n",
        "\n",
        "            outputs = self.forward(X)\n",
        "            loss = loss_fnc(outputs,Y)\n",
        "            loss.backward()\n",
        "\n",
        "            Ypred = torch.argmax(outputs,dim=1)\n",
        "            acc   = float(torch.sum(Ypred == Y))\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "            batch_accurracies.append(acc)\n",
        "            batch_sizes.append(len(Y))\n",
        "\n",
        "            optimizer.step()\n",
        "          \n",
        "          self.validate(validgenerator, batch_size)\n",
        "          train_loss = sum(batch_losses) / len(batch_losses) \n",
        "\n",
        "          ##Early stopping :\n",
        "          \n",
        "          all_losses.append(train_loss)\n",
        "          if train_loss <= self.minloss :\n",
        "            trigger_times = 0\n",
        "            self.minloss = train_loss\n",
        "          else :\n",
        "            trigger_times += 1\n",
        "\n",
        "          if trigger_times >= patience :\n",
        "              print(f\"loss went up {trigger_times} times ! \\nTraining early stopped.\")\n",
        "              break\n",
        "      ###########################\n",
        "\n",
        "    def predict(self,datagenerator,batch_size,device='cpu'):\n",
        "        ##########################\n",
        "        #TODO implement a prediction function that returns the class with highest score for each word in the batch\n",
        "      i = 0\n",
        "      all = []\n",
        "\n",
        "      for (seqX, seqY) in datagenerator.generate_batches(batch_size):\n",
        "        i+=1\n",
        "        \n",
        "        #IMPRESSION DU MOT\n",
        "        letters = decode_sequence(seqX[0], datagenerator.input_idx2sym)\n",
        "        word = \"\".join(letters)\n",
        "        \n",
        "\n",
        "        X = torch.LongTensor(seqX).to(device)\n",
        "        outputs = self.forward(X)\n",
        "        #print(outputs)\n",
        "        Ypred = torch.argmax(outputs)\n",
        "        #print(Ypred)\n",
        "        language = datagenerator.output_idx2sym[Ypred.item()]\n",
        "\n",
        "        all.append((word, language))\n",
        "\n",
        "      shuffle(all)\n",
        "      print(all[100:150])\n",
        "        ##########################\n",
        "\n",
        "    def validate(self,datagenerator,batch_size,device='cpu',save_min_model=False):\n",
        "        #This function cannot be modified \n",
        "\n",
        "        batch_accurracies = []\n",
        "        batch_losses      = []\n",
        "        batch_sizes       = []\n",
        "\n",
        "        device    = torch.device(device)\n",
        "        loss_fnc  = nn.CrossEntropyLoss()\n",
        "\n",
        "        for (seqX,seqY) in datagenerator.generate_batches(batch_size):\n",
        "\n",
        "              with torch.no_grad():   \n",
        "                  X    = torch.LongTensor(seqX).to(device)\n",
        "                  Y    = torch.LongTensor(seqY).to(device)\n",
        "\n",
        "                  Yhat = self.forward(X)\n",
        "                  loss = loss_fnc(Yhat,Y)\n",
        "                  Ypred = torch.argmax(Yhat,dim=1)\n",
        "                  acc   = float(torch.sum(Ypred == Y))\n",
        "                \n",
        "                  batch_losses.append(loss.item())\n",
        "                  batch_accurracies.append(acc)\n",
        "                  batch_sizes.append(len(Y))\n",
        "        \n",
        "        valid_loss = sum(batch_losses) / len(batch_losses) \n",
        "        print('[valid]  mean loss = %f, mean acc = %f'%( valid_loss , sum(batch_accurracies)/sum(batch_sizes)))\n",
        "        if valid_loss < self.minloss:\n",
        "            self.minloss = valid_loss\n",
        "            torch.save(self.state_dict(), 'names_params.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obu2Bib2tZHI"
      },
      "source": [
        "Main program. You are expected to search for hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Tj4rxKtYqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1d4c33-4987-4316-db4c-39ddddb868e3"
      },
      "source": [
        "traing = DataGenerator('name2lang.train')\n",
        "validg = DataGenerator('name2lang.valid',parentgenerator=traing)\n",
        "\n",
        "# model = LanguageIdentifier(traing,2,32,512)\n",
        "# model.train(traing,validg,25,128)\n",
        "\n",
        "model = LanguageIdentifier(traing,2,32,512)\n",
        "model.load_state_dict(torch.load('names_params.pt'))\n",
        "\n",
        "model.predict(traing, 1)\n",
        "\n",
        "#model.validate(validg, 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Linsby', 'Russian'), ('Purnell', 'Russian'), ('Grierson', 'Russian'), ('Makhnenko', 'Russian'), ('Tyrrell', 'English'), ('Baz', 'Arabic'), ('Jachikov', 'Russian'), ('Bokhoven', 'Russian'), ('Podolinsky', 'Russian'), ('Mojin', 'Russian'), ('Yakubonis', 'Russian'), ('Vysokov', 'Russian'), ('Braune', 'German'), ('Lichman', 'English'), ('Deeb', 'Arabic'), ('Ichikawa', 'Japanese'), ('Vaskovsky', 'Russian'), ('Zhilinsky', 'Russian'), ('Norwood', 'English'), ('Allison', 'English'), ('Mikhail', 'Arabic'), ('Sai', 'Chinese'), ('Uzlov', 'Russian'), ('Shamon', 'Arabic'), ('Rooiakkers', 'English'), ('Whitlock', 'English'), ('Tulin', 'Russian'), ('Eneev', 'Russian'), ('Vinaver', 'Italian'), ('Potenza', 'Russian'), ('Tsaregradsky', 'Russian'), ('Filipek', 'Czech'), ('Munyabin', 'Russian'), ('Mukovozov', 'Russian'), ('Bakradze', 'Russian'), ('Dudnakov', 'Russian'), ('Balashov', 'Russian'), ('Hubiev', 'Russian'), ('Shalyugin', 'Russian'), ('Kawatake', 'Japanese'), ('Harlanov', 'Russian'), ('Teufel', 'Russian'), ('Sarumara', 'Japanese'), ('Tsaliev', 'Russian'), ('Onoda', 'Japanese'), ('Ryzhev', 'Russian'), ('Emeshin', 'Russian'), ('Shu', 'Chinese'), ('Borovka', 'Czech'), ('Dzhanakavov', 'Russian')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Nn8FOyjW9TEA",
        "outputId": "a88fe5e6-1e3a-4826-c387-3a5a1b889655"
      },
      "source": [
        "#INTERACTIVE PREDICTIONS \n",
        "\n",
        "#not implemented yet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-443199561c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#num = \"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter number :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    }
  ]
}