{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi-supervized W-S disambiguation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcauzi/Some-programs-I-made/blob/main/Semi%20supervized%20/%20auto-trained%20word-sense%20disambiguation%20model/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbXHJZnbs3Es"
      },
      "source": [
        "# Partie 1 : récupération des embeddings avec gensim\n",
        "\n",
        "Pour l'instant, on les récupère sur ce lien : https://fauconnier.github.io/\n",
        "\n",
        "Word2vec fair à partir du corpus frWac par Mr Jean-Philippe Fauconnier\\n\n",
        "\n",
        "Utilise un corpus de textes tirés du super journal français **Le Monde**\n",
        "\n",
        "Taille des vecteurs d'embedding : 200\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQe0nFqwuadl",
        "outputId": "b8a743b3-d6c0-408e-80dc-3b64a7bacee9"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\", binary=True, unicode_errors=\"ignore\")\n",
        "model.save(\"word2vec.model\")\n",
        "#model.most_similar permet d'afficher les mots les plus similaires à un mot cible dans notre model w2v\n",
        "#model.most_similar(\"voiture\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('véhicule', 0.7415274381637573),\n",
              " ('voitures', 0.7131628394126892),\n",
              " ('automobiliste', 0.6627452373504639),\n",
              " ('garée', 0.6530455946922302),\n",
              " ('camionnette', 0.6363461017608643),\n",
              " ('portière', 0.6300318837165833),\n",
              " ('chauffeur', 0.6250183582305908),\n",
              " ('auto', 0.6175906658172607),\n",
              " ('bagnole', 0.6143242120742798),\n",
              " ('décapotable', 0.6119450330734253)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNhPLZ_8wYO3"
      },
      "source": [
        "# Partie 2 : parsing des données d'apprentissage (premier corpus fourni)\n",
        "\n",
        "Parcour des données de train et constitution d'une liste d'exemples avec leur classe gold pour les 3 verbes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTwg3c1Jwdfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d66f8d-c296-4cb5-b8b8-b6c082e2b7b3"
      },
      "source": [
        "\n",
        "# ---- EXTRACTION DES DATAS SUPERVISEES POUR ABATTRE / AFFECTER / ABORDER -----#\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load(\"word2vec.model\")\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "# from icecream import ic\n",
        "from pprint import pprint\n",
        "\n",
        "with open(\"affecter.deep_and_surf.sensetagged.conll\",\"r\") as file :\n",
        "    with open(\"data\",\"wb\") as data :\n",
        "        content = file.read()\n",
        "        text = []\n",
        "        flex_list =[]\n",
        "        for phrase in content.split(\"\\n\\n\") :\n",
        "            if len(phrase) > 0:\n",
        "                temp = \"\"\n",
        "                sens = \"\"\n",
        "                for line in phrase.split(\"\\n\") :\n",
        "                    tmp = line.split(\"\\t\")\n",
        "                    if tmp[2] == \"affecter\" :\n",
        "                        #J'AI RAJOUTE CETTE CONDITION (car certaines formes fléchies\n",
        "                        #n'ont pas d'embedding donc on ne peut pas intégrer le phrase dans le corpus.)\n",
        "                        if not exists(tmp[1]) :\n",
        "                          break\n",
        "                        #split sur | pour sur # puis on récupère le numéro du sens\n",
        "                        sens = [i for i in tmp[5].split(\"|\") if \"sense\" in i][0].split(\"#\")[1]\n",
        "                        #récupérer la forme fléchie\n",
        "                        if tmp[1] not in flex_list:\n",
        "                          flex_list.append(tmp[1])\n",
        "                    # checker si existe dans embedding\n",
        "                    if exists(tmp[1]):\n",
        "                      temp += tmp[1] + \" \"\n",
        "            # récupère suelement si le sens a un numéro\n",
        "            if len(sens) == 1:\n",
        "              text.append((temp,sens))\n",
        "        pickle.dump(text, data)\n",
        "\n",
        "\n",
        "# POUR DÉSÉRIALIZER\n",
        "with open ('data', 'rb') as data:\n",
        "  train_corpus_sent = pickle.load(data)\n",
        "\n",
        "print({sens[1] for sens in train_corpus_sent})\n",
        "\n",
        "#petite fonction pour regarder les phrases d'un certain sens :\n",
        "for example in train_corpus_sent :\n",
        "  if example[1] == \"3\" :\n",
        "    print(example[0])\n",
        "\n",
        "# print(flex_list)\n",
        "\n",
        "\n",
        "#sens 1 : abattre des batiments / arbres / avions / \n",
        "#sens 2 : abattre des humains / tuer\n",
        "#sens 3 : décourager \n",
        "#sens \"\" : geste d'abattre qqchose (carte, épée)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpDdnhVBSE6_"
      },
      "source": [
        "Extraction des exemples non annotés du corpus Est Républicain\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWU5eOQNwvla",
        "outputId": "c94441f4-f639-45b8-8997-fcb4efe440a9"
      },
      "source": [
        "# ---------------- EXTRACTION DES DATAS NON-SUPERVISEES -----------------------#\n",
        "\n",
        "#extraction des phrases pour corpus non supervisé.\n",
        "# ---------------------- EXTRACTION DES DATAS POUR ABATTRE ---------------------- #\n",
        "import pickle\n",
        "import os\n",
        "# from icecream import ic\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "list = ['aboutir', 'investir', 'traduire', 'témoigner', 'juger', 'justifier',\n",
        "        'viser', 'prononcer', 'accomplir', 'convenir', 'acquérir', 'achever',\n",
        "        'observer', 'adapter', 'admettre', 'entraîner', 'payer', 'respecter',\n",
        "        'affecter', 'demeurer', 'aggraver', 'agir', 'ajouter', 'alimenter',\n",
        "        'coûter', 'relancer', 'préférer', 'appliquer', 'apporter', 'fonder',\n",
        "        'appuyer', 'changer', 'chuter', 'soutenir', 'concevoir', 'interroger',\n",
        "        'confirmer', 'transformer', 'manifester', 'interpeller', 'signer',\n",
        "        'rester', 'tuer', 'indiquer', 'conduire', 'situer', 'aider',\n",
        "        'poursuivre', 'profiter', 'détenir', 'lire', 'contenir', 'dominer',\n",
        "        'noter', 'dater', 'adopter', 'enregistrer', 'intervenir', 'conclure',\n",
        "        'disputer', 'estimer', 'appartenir', 'arriver', 'chercher', 'composer',\n",
        "        'confier']\n",
        "\n",
        "#model = KeyedVectors.load(\"word2vec.model\")\n",
        "\n",
        "def exists(word) :\n",
        "    # in : a word string\n",
        "\t  # out:\n",
        "\t  # checks whether a given word has a corresponding embedding in our w2v model\n",
        "    try:\n",
        "        embedding_vector = model[word]\n",
        "        return True\n",
        "    except: return False\n",
        "\n",
        "\n",
        "def extract(filename, lemma) :\n",
        "    #extracts the sentences with occurences of the lemma from a single file of the corpus Est Républicain\n",
        "    #in : the name of the file containing the sentences / the lemma in str\n",
        "    #out : the list of sentences containing occurences of the lemma \n",
        "    #footnote :: a single sentence is in the format [word_list,{\"flex_form\":str,\"lemma\":str}]\n",
        "    text = []\n",
        "    flex_list =[]\n",
        "    with open(filename, encoding=\"utf8\", errors='ignore') as file :\n",
        "        with open(\"data\",\"wb\") as data :\n",
        "            content = file.read()\n",
        "            for phrase in content.split(\"\\n\\n\") :\n",
        "                contains = False\n",
        "                if len(phrase) > 0:\n",
        "                    for line in phrase.split(\"\\n\") :\n",
        "                        word = line.split(\"\\t\")\n",
        "                        if len(word) > 3 :\n",
        "                            if word[2] == lemma :\n",
        "                                contains = True\n",
        "                                flex = word[1]\n",
        "                                if flex not in flex_list:\n",
        "                                    flex_list.append(flex)\n",
        "                if contains :\n",
        "                    sentence = \"\"\n",
        "                    for line in phrase.split(\"\\n\") :\n",
        "                        if exists(line.split(\"\\t\")[1]):\n",
        "                            sentence += line.split(\"\\t\")[1] + \" \"\n",
        "                    word_list = sentence.split(\" \")\n",
        "                    text.append([word_list, [{\"flex_form\" : flex, \"lemma\" : lemma}]])\n",
        "            return text\n",
        "\n",
        "\n",
        "def extractall_one(dirname, lemma) :\n",
        "    #extracts all sentences containing an occurence of the lemma in a set of files from the corpus Est Républicain\n",
        "    #in : the name of the directory containing the set of files\n",
        "    #out : the list of sentences containing occurences of the lemma \n",
        "    text = []\n",
        "    i = 0\n",
        "    for filename in os.listdir(dirname):\n",
        "        i+=1\n",
        "        new = extract(dirname + \"/\" + filename, lemma)\n",
        "        text = text + new\n",
        "        print(f\"extraction occurences '{lemma}' fichier {i}/27\")\n",
        "    return text\n",
        "\n",
        "\n",
        "#print(extractall_one(\"estrepublicain\", \"affecter\")[0])\n",
        "\n",
        "def extractall_all(dirname, lemma_list) :\n",
        "    #extracts all sentences containing an occurence of one of the lemmas in a lemma list\n",
        "    #in : the name of the directory containing the set of files from the corpus\n",
        "    #out : the liste of sentences containing occurences of one of the lemmas\n",
        "    full_list = []\n",
        "    i = 0\n",
        "    for lemma in lemma_list :\n",
        "        i+=1\n",
        "        full_list.extend(extractall_one(dirname, lemma))\n",
        "        print(f\"fin de l'extraction occurences '{lemma}' : {i}/{len(lemma_list)}\")\n",
        "    return full_list\n",
        "\n",
        "# test = extractall_all(\"estrepublicain\", list[0 : 3])\n",
        "\n",
        "#serialization functions :\n",
        "file_name = \"all#noclass.pkl\"\n",
        "# open_file = open(file_name, \"wb\")\n",
        "# pickle.dump(extractall_all(\"estrepublicain\", list), open_file)\n",
        "# open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(file_name, \"rb\")\n",
        "list = pickle.load(open_file)\n",
        "open_file.close()\n",
        "print(list[0])\n",
        "print(list[1000])\n",
        "print(list[10000])\n",
        "print(len(list))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "incident qui n' affecte pas le véritable amateur comme le rappelle le commissaire-priseur « ont bourré leurs d' électronique et c' est toujours avec une légère suspicion qu' on pénètre dans une telle voiture \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VobuGkKeTunS"
      },
      "source": [
        "Extraction des données annotées du corpus FSE entier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRMx048NTyo0"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "import re\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "def extract_data(data_filename,annotations_filename):\n",
        "\n",
        "\t#collecting gold annotations\n",
        "\twith open(annotations_filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\t\n",
        "\t# dict list to garner relevant information\n",
        "\tword_dicts = {}\n",
        "\n",
        "\tfor line in lines:\n",
        "\t\tall_data = line.split('__')\n",
        "\t\ttoken_id = all_data[0][:-1]\n",
        "\t\tval_inf = all_data[1].split('_')\n",
        "\t\tword_dicts[token_id] = { \"lemma\" : val_inf[2], \"word sense\" : val_inf[1], \"sentence_id\" : token_id[1:]}\n",
        "\n",
        "\t# word_dicts format : {token_id : {\"lemma\":...,\"word sense\" : ...,\"sentence_id\";...}}\n",
        "\n",
        "\t# xml parsing of the FSE file\n",
        "\tsoup = Soup(open(data_filename),features=\"lxml\")\n",
        "\n",
        "\t# list of sentences in the following format for a single sentence : [word_list,{}]\n",
        "\tsentences = []\n",
        "\tfor sentence in soup.findAll(\"sentence\"):\n",
        "\t\tinstances = sentence.findAll(\"instance\")\n",
        "\t\twords = sentence.text\n",
        "\t\tsentences.append([words.split(),[{\"flex_form\" : instance.text, \"id\" : str(instance['id']),\"lemma\" : word_dicts[instance['id']].get('lemma'),\"word sense\" : word_dicts[instance['id']].get('word sense')} for instance in instances]])\n",
        "\n",
        "\t# in order to partition the data by lemma, storing data in the the wsd_data dict\n",
        "\twsd_data = {}\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor item in sentence[1]:\n",
        "\t\t\twsd_data[item[\"lemma\"]] = wsd_data.get(item[\"lemma\"],[])+[sentence]\n",
        "    \n",
        "\treturn wsd_data\n",
        "\n",
        "# uncomment to run the extraction\n",
        "#fse_data = extract_data(\"/Users/leolabat/Downloads/FSE-1.1-191210/FSE-1.1.data.xml\",\"/Users/leolabat/Downloads/FSE-1.1-191210/FSE-1.1.gold.key.txt\")\n",
        "\n",
        "# uncomment to retrieve the pickled version of the dictionarized FSE data\n",
        "# with open('/Users/leolabat/Desktop/WSD_project/fse_data.pickle','rb') as handle:\n",
        "# \tfse_data = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwSQ6ykPv5xp"
      },
      "source": [
        "# Partie 3 : classifieur supervisé\n",
        "\n",
        "Permet de créer des vecteurs de contextes comme addition des embeddings des mots dans un contexte autour du mot cible (forme fléchie du mot cible comprise). \n",
        "\n",
        "- deux classifieurs permettant de prédire le sens du mot cible dans un contexte non-vu à l'apprentissage (SVM et MLP) \n",
        "- une partition du corpus en train / test pour évaluer les performances du classifieur.\n",
        "- des création de tableaux pour stocker les résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKg6lCDAwJe5"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import gzip\n",
        "import gensim\n",
        "import logging\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import statistics\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from statistics import pstdev\n",
        "\n",
        "# collecting the dictionary with gold annotations from FSE data\n",
        "with open('fse_data.pickle','rb') as handle:\n",
        "    fse_data = pickle.load(handle)\n",
        "\n",
        "# collecting embedding model\n",
        "model = KeyedVectors.load(\"word2vec.model\")\n",
        "\n",
        "#collecting new data from the sskmeans :\n",
        "file_name = \"newdata.pkl\"\n",
        "open_file = open(file_name, \"rb\")\n",
        "newdata = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "# custom-made functions\n",
        "def cosine_similarity(vec1, vec2) :\n",
        "\t# in : two vectors of similar shape\n",
        "    # out : cosine similarity between the two vectors\n",
        "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
        "\n",
        "def exists(word) :\n",
        "\t# in : a word string\n",
        "\t# out:\n",
        "\t# checks whether a given word has a corresponding embedding in our w2v model\n",
        "    try:\n",
        "        embedding_vector = model[word]\n",
        "        return True\n",
        "    except: return False\n",
        "\n",
        "def train_corpus(sentences,lemma, window=4):\n",
        "\t# in : a list of sentences and a string for the lemma\n",
        "\t# footnote :: a single sentence is in the format [word_list,{\"flex_form\":str,\"lemma\":str}]\n",
        "\t# out : a list of pairs : (context_vector,gold_class)\n",
        "\n",
        "\tvectorized_train = []\n",
        "\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor flex_form in sentence[1]:\n",
        "\t\t\tif flex_form.get(\"lemma\") == lemma:\n",
        "\t\t\t\tvectorized_train.append((get_context(sentence[0], flex_form.get('flex_form'), window), flex_form.get('word sense')))\n",
        "\treturn vectorized_train\n",
        "\n",
        "def vectorize(sentences,lemma):\n",
        "\t# in : a list of sentences (format : [word_list,{\"flex_form\":str,\"lemma\":str}]) and a str for the lemma\n",
        "\t# out : a list of vectors corresponding to the contextualised vector representations of lemma occurrences\n",
        "\n",
        "    vectorized = []\n",
        "    for sentence in sentences:\n",
        "        for flex_form in sentence[1]:\n",
        "            if flex_form.get(\"lemma\") == lemma:\n",
        "                try:\n",
        "                    vectorized.append(get_context(sentence[0],flex_form.get('flex_form')))\n",
        "                except: continue\n",
        "    return vectorized\n",
        "\n",
        "\n",
        "\n",
        "def get_context(word_list,flex_form,window=4):\n",
        "\t# in : word list corresponding to the sentence the target word was taken from, the occurrence itself (flex_form), the context window size\n",
        "\t# out : a single vector corresponding to the in-place addition of fixed embeddings, thus encapsulating the target word\n",
        "\n",
        "\t# checking if there is an issue with the flex_form\n",
        "    if flex_form in word_list:\n",
        "        index = word_list.index(flex_form)\n",
        "    else:\n",
        "        print(\"flex_form not in word list\")\n",
        "        raise ValueError(\"flex_form not in word list\")\n",
        "\n",
        "    context = []\n",
        "    for i in range(1, window+1):\n",
        "        if not index-i < 0 :\n",
        "            #if the word is unavailable in our embeddings it won't be taken into account (=vectorized into a null vector)\n",
        "            if exists(word_list[index-i]) : context.append(model[word_list[index-i]])\n",
        "        if index+i < len(word_list) :\n",
        "            if exists(word_list[index+i]) : context.append(model[word_list[index+i]])\n",
        "    # if the flex_form exists, it is added as well (obviously)\n",
        "    if exists(flex_form):\n",
        "    \tcontext.append(model[word_list[index]])\n",
        "\n",
        "    #first context word as initialization\n",
        "    context_embed = context[0]\n",
        "    # in-place addition of all the other vectors\n",
        "    for i in range(1, len(context)) :\n",
        "        context_embed = np.add(context_embed, context[i])\n",
        "    return context_embed\n",
        "\n",
        "def baseline(corpus) :\n",
        "\t# in : annotated corpus in its vectorized form(!)\n",
        "\t# out : a list of tuples (word sense,word sense frequency)\n",
        "\n",
        "\tY = []\n",
        "\tfor example in corpus :\n",
        "\t\tY.append(example[1])\n",
        "\t\tc = Counter(Y)\n",
        "\t#print(\"Most frequent baseline : \", round(max([(c[i] / len(Y) * 100.0) for i in c]), 2), \"%\")\n",
        "\treturn round(max([(c[i] / len(Y) * 100.0) for i in c]), 2)\n",
        "\n",
        "\n",
        "##\n",
        "##\n",
        "## PLAIN CLASSIFICATION\n",
        "##\n",
        "##\n",
        "\n",
        "def SVM_WSD(data,lemma,partition,n_trials,nb_iter_svm):\n",
        "    # in :\n",
        "    #      data : lemma dictionary with list of annotated data as values\n",
        "    #      lemma : string corresponding to the lemma undergoing disambiguation\n",
        "    #      partition : float of value between 0 and 1 (excluded) to partition the data into train/test\n",
        "    #      n_trials : number of trials\n",
        "    #      nb_iter_svm : max_iter parameter for the svm\n",
        "    # out : average accuracy of the SVM algorithm on a fraction of the annotated data\n",
        "\n",
        "    # to avoid overwriting our data (precautionary measure)\n",
        "    examples = data.get(lemma).copy()\n",
        "\n",
        "    # used to partition the data into a train_set and a test_set\n",
        "    size = len(examples)\n",
        "    tr = int(size*partition)\n",
        "\n",
        "    # vectorization of the training set\n",
        "    training_set = train_corpus(examples,lemma)\n",
        "\n",
        "    print(f'Taille des données annotées : {size} exemples')\n",
        "\n",
        "    baseline(training_set)\n",
        "\n",
        "    accuracy_list = []\n",
        "    for i in range(n_trials) :\n",
        "        # randomize the parition of the annotated data\n",
        "        random.shuffle(training_set)\n",
        "\n",
        "        X, Y = [], []\n",
        "        for example in training_set[0:tr] :\n",
        "            X.append(example[0])\n",
        "            Y.append(example[1])\n",
        "\n",
        "        if len(Y)<=1 :\n",
        "            break\n",
        "\n",
        "        X_test, Y_gold = [], []\n",
        "        for example in training_set[tr:] :\n",
        "            X_test.append(example[0])\n",
        "            Y_gold.append(example[1])\n",
        "\n",
        "        # Uncomment to add the new data from the SSK-mean as train examples\n",
        "        # for i in range(len(newdata[lemma])) :\n",
        "        #     X.append(newdata[lemma][i][0])\n",
        "        #     Y.append(newdata[lemma][i][1])\n",
        "\n",
        "\n",
        "      #max iter un peu plus grand que 1000 pour ne pas avoir de warning de non-convergence\n",
        "        lin_clf = svm.LinearSVC(max_iter=nb_iter_svm)\n",
        "        lin_clf.fit(X, Y)\n",
        "        Y_pred = lin_clf.predict(X_test)\n",
        "\n",
        "        accuracy_list.append(metrics.accuracy_score(Y_gold, Y_pred))\n",
        "    return accuracy_list\n",
        "    #print(f\"SVM accuracy for \\\"{lemma}\\\" : {round(statistics.mean(accuracy_list)*100, 2)}%\")\n",
        "\n",
        "\n",
        "\n",
        "def data(lemma, window):\n",
        "    #collects data for various hyperparameters : lemma, context window\n",
        "    data = []\n",
        "    for num in window:\n",
        "        train = train_corpus(fse_data.get(lemma).copy(), lemma, num)\n",
        "        #if len(data) == 0:\n",
        "        acc = SVM_WSD(fse_data, lemma, 0.8, 10, 10000)\n",
        "        data.append([f\"{lemma} / {baseline(train)}\", num,\n",
        "        f\"{round(max(acc)*100, 2)}%\",\n",
        "        f\"{round(min(acc)*100, 2)}%\",\n",
        "        f\"{round(statistics.mean(acc)*100, 2)}%\",\n",
        "        round(pstdev(acc), 4)])\n",
        "\n",
        "    return data\n",
        "\n",
        "# table creation :\n",
        "columns = [\"lemma / baseline\", \"window\", \"max\", \"min\", \"mean\", \"standard deviation\"]\n",
        "window = [4]\n",
        "# list of the 18 words used for semi-supervised auto-trained classification\n",
        "list = ['traduire', 'viser', 'prononcer', 'convenir', 'achever', 'adapter',\n",
        "        'alimenter', 'coûter', 'relancer', 'appuyer', 'chuter', 'confirmer',\n",
        "        'manifester', 'conduire', 'aider', 'détenir', 'contenir', 'dater', 'confier']\n",
        "\n",
        "tableau = pd.DataFrame(data=data(list[0], window), columns = columns)\n",
        "for lemma in list[1:] :\n",
        "    df1 = pd.DataFrame(data=data(lemma, window) , columns = columns)\n",
        "    tableau = pd.concat([tableau,df1], ignore_index=True)\n",
        "    print(tableau)\n",
        "tableau.to_csv(\"tables/supervized_SVM19.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5mjkAi4ainW"
      },
      "source": [
        "Classifieur par réseau de neurones à couche cachée (Multi-Layered Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3u3FQzcajGb"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_accuracy = []\n",
        "\n",
        "for i in range(100) :\n",
        "\n",
        "  random.shuffle(train_corpus)\n",
        "\n",
        "  X, Y = [], []\n",
        "  for example in train_corpus[0:140] :\n",
        "    X.append(example[0])\n",
        "    Y.append(example[1])\n",
        "\n",
        "  X_test, Y_gold = [], []\n",
        "  for example in train_corpus[140:165] :\n",
        "    X_test.append(example[0])\n",
        "    Y_gold.append(example[1])\n",
        "  #max iter un peu plus grand que 1000 pour ne pas avoir de warning de non-convergence\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=100,activation='tanh',solver='sgd',random_state=1,max_iter=5000)\n",
        "  mlp.fit(X, Y)\n",
        "  Y_pred = mlp.predict(X_test)\n",
        "\n",
        "  mlp_accuracy.append(metrics.accuracy_score(Y_gold, Y_pred))\n",
        "\n",
        "print(f\"mlp accuracy : {statistics.mean(mlp_accuracy)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9IgzHu5Arz6"
      },
      "source": [
        "Partie 5 : clustering semi-supervisé "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdYT4aEfVo0h"
      },
      "source": [
        "SSK-mean pour l'extraction de nouveaux exemples de train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL8Q2EQfAvZX"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from statistics import pstdev\n",
        "from wsd_plain_classification import *\n",
        "\n",
        "with open('fse_data.pickle','rb') as handle:\n",
        "    fse_data = pickle.load(handle)\n",
        "\n",
        "def get_key(my_dict,val):\n",
        "    for key, value in my_dict.items():\n",
        "        if val == value:\n",
        "            return key\n",
        "    return \"key doesn't exist\"\n",
        "\n",
        "class SSK_Means():\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data,labeled_data,labels):\n",
        "\n",
        "    \t# initialisation d'un dictionnaire de centroïdes\n",
        "        self.centroids = {}\n",
        "\n",
        "        #récupération des données étiquetées\n",
        "        self.labeled_data = labeled_data\n",
        "        # récupération des étiquettes gold\n",
        "        self.gold_labels = set(labels)\n",
        "        # agrégation des clusters connus\n",
        "        self.clusters = {}\n",
        "        for label in self.gold_labels:\n",
        "        \tself.clusters[label] = []\n",
        "\n",
        "        # attribution des données étiquettées aux clusters gold\n",
        "        for featset,label in zip(labeled_data,labels):\n",
        "        \tself.clusters[label].append(featset)\n",
        "\n",
        "        # calcul des centroïdes à partir des premières données\n",
        "        for label in self.gold_labels:\n",
        "            self.centroids[label] = np.average(self.clusters[label],axis=0)\n",
        "\n",
        "        # pour chaque epoch\n",
        "        for i in range(self.max_iter):\n",
        "\n",
        "        \t# initialisation d'un dictionnaire pour les appariements aux clusters\n",
        "            self.classifications = {}\n",
        "            #print(self.gold_labels)\n",
        "            # initialisation de listes comme valeurs pour k clés\n",
        "            for label in self.gold_labels:\n",
        "                #print(f'label : {label}')\n",
        "                self.classifications[label] = []\n",
        "\n",
        "            for featureset in data:\n",
        "                distances = {centroid : np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids}\n",
        "                classification = get_key(distances, min(distances.values()))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            for featset,label in zip(labeled_data,labels):\n",
        "        \t    self.classifications[label].append(featset)\n",
        "\n",
        "            # sauvegarde des barycentres ayant servi à la classification de l'epoch\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            # recalcul des nouveaux barycentres à partir de la moyenne des valeurs qu'ils partitionnent\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    #print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = {centroid : np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids}\n",
        "        classification = get_key(distances,min(distances.values()))\n",
        "        return classification\n",
        "\n",
        "hc_path = \"all#noclass.pkl\"\n",
        "\n",
        "with open(hc_path,'rb') as handle:\n",
        "    huge_corpus = pickle.load(handle)\n",
        "\n",
        "# turn the unclassified data into a lemma-based dictionary\n",
        "lemma_data = {}\n",
        "for sentence in huge_corpus:\n",
        "    for item in sentence[1]:\n",
        "        lemma_data[item[\"lemma\"]] = lemma_data.get(item[\"lemma\"],[])+[sentence]\n",
        "\n",
        "def SSKMeans_classifier(known_data,unknown_data,lemma,n_trials,partition):\n",
        "\n",
        "    # récupération des données connues pour le classifieur\n",
        "    known = known_data[lemma].copy()\n",
        "    train = train_corpus(known,lemma)\n",
        "    word_sense_nb = len(set([lab for item,lab in train]))\n",
        "\n",
        "    #vectorizing unknown data\n",
        "    data = vectorize(unknown_data[lemma],lemma)\n",
        "\n",
        "\n",
        "    #pas de partition des données en train et test car test inutile\n",
        "    random.shuffle(train)\n",
        "    training_set = train\n",
        "    labeled_data = []\n",
        "    labels = []\n",
        "    for example,label in training_set:\n",
        "        labeled_data.append(example)\n",
        "        labels.append(label)\n",
        "\n",
        "    Kmeans = SSK_Means()\n",
        "    Kmeans.fit(data,labeled_data,labels)\n",
        "    predicted = []\n",
        "    for example in data:\n",
        "        pred = Kmeans.predict(example)\n",
        "        predicted.append((example, pred))\n",
        "    return predicted\n",
        "\n",
        "#constitution d'un disctionnaire pour récolter les nouveaux exemples \n",
        "lemmadic = {}\n",
        "lemmalist = ['traduire', 'viser', 'prononcer', 'convenir', 'achever', 'adapter', 'alimenter', 'coûter', 'relancer', 'appuyer', 'chuter', 'confirmer', 'manifester', 'conduire', 'aider', 'détenir', 'contenir', 'dater', 'confier']\n",
        "for lemma in lemmalist :\n",
        "    lemmadic[lemma] = SSKMeans_classifier(fse_data,lemma_data,lemma,100,0.8)\n",
        "\n",
        "#sérialisation\n",
        "file_name = \"newdata.pkl\"\n",
        "open_file = open(file_name, \"wb\")\n",
        "pickle.dump(lemmadic, open_file)\n",
        "open_file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}